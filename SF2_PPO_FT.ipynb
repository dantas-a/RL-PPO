{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b16db-f9f7-43ac-8667-859a36bb21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permet de jouer au jeu\n",
    "import retro\n",
    "# Permet de ralentir la vitesse \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00358d8f",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc1f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import MultiBinary, Box\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49926cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environnement Customisé\n",
    "class StreetFighter(Env) : \n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        # A fixer inférieure à 200\n",
    "        self.taille_reduite = 84\n",
    "        # Specification de l'espace des actions et de l'espace d'observation\n",
    "        # low = 0, high = 255 : couleur pixels par défaut\n",
    "        # shape : shape de la sortie par défaut (hauteur, largeur, Gris)\n",
    "        self.observation_space = Box(low=0,high=255,shape=(self.taille_reduite,self.taille_reduite,1),dtype=np.uint8)\n",
    "        # action_space = MultiBinary(12) : 12 touches possibles et combinables pour faire des coups spéciaux\n",
    "        self.action_space = MultiBinary(12)\n",
    "        # Lancer une instance du jeu et ne permet que les combinaisons valides de boutons\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis', use_restricted_actions = retro.Actions.FILTERED)\n",
    "    \n",
    "    def step(self,action):\n",
    "        # Faire une étape \n",
    "        observation, reward, done, info = self.game.step(action)\n",
    "        observation = self.preprocess(observation)\n",
    "        \n",
    "        # Fonction de récompense\n",
    "        reward = info['score'] - self.score\n",
    "        self.score = info['score']\n",
    "        \n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def render(self,*args,**kwargs):\n",
    "        self.game.render()\n",
    "    \n",
    "    def reset(self):\n",
    "        # Remet le jeu à zéro\n",
    "        observation = self.game.reset()\n",
    "        # Preprocess l'image obtenue\n",
    "        observation = self.preprocess(observation)\n",
    "        # Cette variable va permettre de stocker la récompense obtenue pour la partie\n",
    "        self.score = 0\n",
    "        return observation\n",
    "    \n",
    "    def preprocess(self,observation):\n",
    "        # Transformation de l'image RGB en nuance de gris => Entraînement plus rapide\n",
    "        image_gris = cv2.cvtColor(observation,cv2.COLOR_RGB2GRAY)\n",
    "        # Modifier la taille de l'image => Entraînement plus rapide\n",
    "        image_retaillee = cv2.resize(image_gris,(self.taille_reduite,self.taille_reduite), interpolation = cv2.INTER_CUBIC)\n",
    "        # Specificité pour stable_baselines\n",
    "        image_retaillee_gris_finale = np.reshape(image_retaillee,(self.taille_reduite,self.taille_reduite,1))\n",
    "        return image_retaillee_gris_finale\n",
    "    \n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1de8b0",
   "metadata": {},
   "source": [
    "## HyperParamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b55f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188274ff-6b20-4446-93e8-321eddc02b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e6b46d-f3b8-48e0-8140-eb7901a305fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_ppo(trial):\n",
    "    return {\n",
    "        'n_steps':trial.suggest_int('n_steps',2048,8192),\n",
    "        'gamma':trial.suggest_loguniform('gamma',0.8,0.9999),\n",
    "        'learning_rate':trial.suggest_loguniform('learning_rate',1e-9,1e-4),\n",
    "        'clip_range':trial.suggest_uniform('clip_range',0.1,0.4),\n",
    "        'gae_lambda':trial.suggest_uniform('gae_lambda',0.8,0.99)\n",
    "    }\n",
    "\n",
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        # Récupère les hyperparamètres pour le modèle\n",
    "        model_params = optimize_ppo(trial)\n",
    "\n",
    "        # Crée un environnement\n",
    "        env = StreetFighter()\n",
    "        # Permet d'extraire la recompense moyenne et la longueur moyenne d'un episode\n",
    "        env = Monitor(env,LOG_DIR)\n",
    "        #Nécessaire pour Stable Baselines\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        #Empile 4 images consécutives pour donner la perception de mouvement\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "        # Déclaration d'une nouvelle instance de l'algorithme RL avec les paramètres donnés par Optuna\n",
    "        model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        #Entraînement du modèle\n",
    "        model.learn(total_timesteps=50000)\n",
    "        #model.learn(total_timesteps=100000)\n",
    "\n",
    "        # Evaluation du modèle sur 5 partie différentes\n",
    "        mean_reward, _ = evaluate_policy(model,env,n_eval_episodes=1)\n",
    "        env.close()\n",
    "\n",
    "        # Sauvegarde du modèle\n",
    "        SAVE_PATH = os.path.join(OPT_DIR,'trial_{}_best_model_ppo'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "\n",
    "        return mean_reward\n",
    "        \n",
    "    except Exception as e:\n",
    "        return -1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252ead7-b97d-4d9d-945d-a36b44c55673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On veut maximiser la récompense\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# On va chercher le meilleur set d'hyperparamètres\n",
    "study.optimize(optimize_agent,n_trials=10,n_jobs=1)\n",
    "#study.optimize(optimize_agent,n_trials=100,n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de0113-61e8-45e5-95e0-703a487701e4",
   "metadata": {},
   "source": [
    "## Définition du CallBack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7376bdc-a43e-4362-8e35-a012a4036193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f6e3c2-a9a5-4ad0-8899-65bd0ebfe427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self,check_freq,save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback,self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path,exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path,'best_model_ppo_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44433b46-445c-4085-b8b2-9a8ebd8bd563",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fbb294-12d1-4fd6-bfc7-ff3d3f3e5d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=50000, save_path = CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf819e13-c086-46cd-ae06-66cb6687adea",
   "metadata": {},
   "source": [
    "## Préparation de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eec3b5-b064-4863-a744-69ca4394b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crée un environnement\n",
    "env = StreetFighter()\n",
    "# Permet d'extraire la recompense moyenne et la longueur moyenne d'un episode\n",
    "env = Monitor(env,LOG_DIR)\n",
    "#Nécessaire pour Stable Baselines\n",
    "env = DummyVecEnv([lambda: env])\n",
    "#Empile 4 images consécutives pour donner la perception de mouvement\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49c934-6641-4ffb-a0ae-bf48e2c6ee52",
   "metadata": {},
   "source": [
    "## Entrainement du modèle avec les paramètres sélectionné via Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399ff72-1a6c-46a9-9478-cd793d1a63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des meilleurs paramètres\n",
    "model_params = study.best_params\n",
    "# Pour avoir des batch de bonnes tailles\n",
    "model_params['n_steps']= int(model_params['n_steps']/64)*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c53d2-1899-4171-bb07-ff03c83f4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du modèle avec les meilleurs hyperparam\n",
    "model = PPO('CnnPolicy',env,tensorboard_log=LOG_DIR,verbose=1,**model_params)\n",
    "\n",
    "model.learn(total_timesteps = 5000000, callback = callback)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PBIA_Env",
   "language": "python",
   "name": "pbia_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
